To create a reinforcement learning model that predicts whether to use GPT-3 or DALL-E-2 based on text embeddings, we can follow these steps:

1. Prompt the user for messages.
2. Generate embeddings for the provided text.
3. Feed the embeddings to a model that predicts which service (GPT-3 or DALL-E-2) is better suited to handle the user's message.
4. Use the predicted model to respond to the user.
5. Prompt the user to indicate if the selected model was incorrect.
6. Use the `incorrect` signal to train the model.

Below is a simplified example of how you might implement this in Python. Note that this is a high-level example and may require additional details and error handling for a production system.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel, BertTokenizer
import random

# Define a simple neural network for classification
class ModelSelector(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(ModelSelector, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.softmax(out)
        return out

# Load pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = BertModel.from_pretrained(model_name)

# Initialize the model, loss function, and optimizer
input_dim = 768  # BERT embedding size
hidden_dim = 128
output_dim = 2  # Two classes: GPT-3 and DALL-E-2
model = ModelSelector(input_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Function to generate embeddings
def generate_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt')
    with torch.no_grad():
        outputs = bert_model(**inputs)
    embeddings = outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token
    return embeddings

# Function to predict the best model
def predict_model(embeddings):
    model.eval()
    with torch.no_grad():
        outputs = model(embeddings)
    _, predicted = torch.max(outputs, 1)
    return predicted.item()

# Function to train the model
def train_model(embeddings, label):
    model.train()
    optimizer.zero_grad()
    outputs = model(embeddings)
    loss = criterion(outputs, torch.tensor([label]))
    loss.backward()
    optimizer.step()

# Main loop
while True:
    # Prompt the user for a message
    user_message = input("Enter your message: ")

    # Generate embeddings
    embeddings = generate_embeddings(user_message)

    # Predict the best model
    predicted_model = predict_model(embeddings)
    model_name = "GPT-3" if predicted_model == 0 else "DALL-E-2"
    print(f"Predicted model: {model_name}")

    # Simulate using the predicted model (for demonstration purposes)
    if model_name == "GPT-3":
        response = "This is a response from GPT-3."
    else:
        response = "This is an image generated by DALL-E-2."

    print(response)

    # Prompt the user to indicate if the selected model was incorrect
    feedback = input("Was the selected model incorrect? (yes/no): ").strip().lower()
    if feedback == "yes":
        correct_model = 1 - predicted_model  # The other model is correct
        train_model(embeddings, correct_model)
        print("Model has been updated based on feedback.")
    else:
        print("Model selection was correct.")
```

### Explanation:
1. **Model Definition**: A simple neural network is defined to classify between GPT-3 and DALL-E-2.
2. **BERT Embeddings**: The BERT model is used to generate embeddings for the input text.
3. **Prediction**: The neural network predicts which model to use based on the embeddings.
4. **User Feedback**: The user provides feedback on whether the selected model was correct.
5. **Training**: If the user indicates the model was incorrect, the neural network is updated using the feedback.

This example is a starting point and can be expanded with more sophisticated techniques, better error handling, and integration with actual GPT-3 and DALL-E-2 APIs.