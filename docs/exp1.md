I've designed an experiment I would like your input on:

## Experiment Setup & Objectives
- Choose the most suited model for robust embeddings
- Use a vector-controlled text generator
- Use an in-memory buffer to hold partially generated text
- Train a transformer model to predict vectors that can be given to the vector-controlled text generator, which will add a token corresponding with the generated vector to the buffer
- Cache embeddings for performance
- Except white space and punctuation, my "tokens" will essentially be whole words.

## Model Inputs
- A target vector
- A summary embedding vector of the text made up of tokens in the buffer

## Model Outputs
- Embedding vector of the next token to add to the buffer
- Boolean indicating completion, i.e. no new tokens should be added to the buffer and the results can be compared with the target vector

## Training Details
- Select a training data set of english sentences
- Generate embeddings for each sentence that will serve as target embedding vectors
- Given a target vector, the model should predict the tokens needed to get a new summary vector that is as close to the target as possible

## Vector-controlled Generator
- The code below is an example vector controlled generator that is implemented with cosine similarity and text-embedding-3-large just to convey the general idea.
- Whole words from the training set can be indexed via learn_token so all original words from the training set are indexed
- New words can be added to the text buffer by invoking next_token with an "intention" vector generated by the model
- The summary vectors needed by the model can be gotten by invoking summarize_buffered_text()
- When the completion boolean is output by the model, empty_token_buffer() can be used to reset the buffer and return the buffered text
```python
import asyncio
import faiss
import logging
import numpy as np
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

async_openai_client = AsyncOpenAI()


class VectorControlledGenerator:
    def __init__(
            self,
            embedding_dimensions: int = 3072,  # Based on text-embedding-3-large
    ):
        self.embedding_dimensions = embedding_dimensions
        self.known_tokens = []
        self.token_buffer = []
        self.token_index = faiss.IndexIDMap(faiss.IndexFlatIP(embedding_dimensions))

    async def empty_token_buffer(self):
        buffered_text = "".join(self.token_buffer)
        self.token_buffer = []
        return buffered_text

    async def get_text_vector(self, text: str):
        response = await async_openai_client.embeddings.create(
            dimensions=self.embedding_dimensions,
            model="text-embedding-3-large",
            input=text,
            encoding_format="float"
        )
        vector = np.array([response.data[0].embedding], dtype=np.float32)
        faiss.normalize_L2(vector)
        return vector

    async def learn_token(self, token: str):
        if token not in self.known_tokens:
            self.known_tokens.append(token)
        token_id = self.known_tokens.index(token)
        vector = await self.get_text_vector(token)
        await asyncio.to_thread(
            self.token_index.add_with_ids, vector, np.array([token_id], dtype=np.int64)
        )

    async def next_token(self, intention_vector, min_sim_score: float = 0.9):
        sim_scores, neighbors = await asyncio.to_thread(self.token_index.search, intention_vector, 1)
        matched = False
        for rank, (token_id, sim_score) in enumerate(zip(neighbors[0], sim_scores[0]), 1):
            if token_id != -1 and sim_score > min_sim_score:  # -1 means no match
                token =  self.known_tokens[token_id]
                logger.info(f"{rank:>2}. token='{token}' sim={sim_score:.4f}")
                self.token_buffer.append(token)
                matched = True
                break
        if not matched:
            # This should force a completion so we can learn from this mistake
            raise RuntimeError("failed to match token")

    async def summarize_buffered_text(self):
        buffered_text = "".join(self.token_buffer)
        return await self.get_text_vector(buffered_text)
```

There are some problems with this approach, so suggest ways to address them.

I know text-embedding-3-large uses pooling, which drops order-specific features that are important for my task.
I probably need something like an order-aware bi-encoder so that I can have reusable order-aware embeddings.
I assumed text-embedding-3-large is able to capture a lot more semantic nuances due to its large dimensionality but if that's not the case, suggest something better.
I probably also want to impose a max limit on the buffer size since above ~40 tokens the single-vector summaries from any models degrade fast.

Does it make sense to use one model for selecting the words that should go into the buffer and another model to handle ordering?

Don't focus on reviewing the example code - that's just to give you a general idea of how a "vector-controlled text generator" might work.
This will only be used for research so don't worry about performance or cost at production scale - this model will only have one or two human users at inference time.
My primary object is actually to validate the concept of a vector-controlled switch, where some agent can learn a way to provide an "intention" i.e. a semantic embedding to trigger some action that alters it's environment.
Text generation seems like a use case that would stretch this concept to its limit. Potentially, I might also try this idea for other tasks where the "switch" or generator in this case has fewer choices but more semantically meaningful vectors.

Take as many steps in your chain of thought as you can to get the best possible answer.

---

# Response 1

*(For a shared reference point: I see that it’s **Sunday 18 May 2025, \~08:10 AM PDT (UTC-07)**.  Let me know if your clock says otherwise.)*

---

### 1.  Core frictions in the current design

| Pain-point                                               | Why it hurts                                                                                                                         | Pragmatic fix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Notes                                                                         |
| -------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |
| **Order lost in pooled embeddings**                      | `text-embedding-3-large` mean-pools sub-token vectors, so “dog bites man” ≈ “man bites dog”. Good for search, lethal for generation. | **Train / fine-tune an order-aware bi-encoder.**  Two recent, off-the-shelf baselines that *do* punish word swaps are: <br>• **Predicate-Argument Bi-Encoder (PABiE)** — injects predicate/argument spans before pooling, beating vanilla SBERT on PAWS and other order-stress datasets  <br>• **E5-base-v2** — retains positional information by using RoPE and keeping the final-token (EOS) vector instead of mean pooling.  It’s open-source and half the size of TE-3-large ([Hugging Face][1]) | PABiE is still sentence-level; you will need token-level heads (see §2).      |
| **Single summary vector degrades >≈40 tokens**           | Empirically, cosine distances saturate as you cram more semantics into a single vector.                                              | **Hierarchical summarisation.**  Keep a running mean for ≤N tokens (say 16) and roll older tokens into a *higher-level* vector.  Prediction target becomes a *stack* of vectors.  Think of it as a tiny “vector tree” not a single root.                                                                                                                                                                                                                                                             | Cheap to implement; no extra FAISS calls.                                     |
| **Greedy retrieval from FAISS is brittle**               | If the intention vector lands between tokens, `.search(...,1)` will miss; you raise an exception and force completion.               | Return top-k (k≈8) then score candidates in a cross-encoder (“reranker”).  Latency stays sub-10 ms with a distilled cross-encoder.                                                                                                                                                                                                                                                                                                                                                                   |                                                                               |
| **Training objective conflates *which* word and *when*** | The model must learn lexical choice and ordering in one shot.                                                                        | **Two-stage controller**: <br>1. *Selector* (bi-encoder) outputs a **distribution over vocabulary vectors** (which word) <br>2. *Placer* (lightweight transformer or pointer-network) decides *position* relative to current buffer.                                                                                                                                                                                                                                                                 | See §3 for concrete recipes.                                                  |
| **Cache pressure / OOM on token vectors**                | Storing a 3 k-dim vector per token in Python lists will eat RAM fast.                                                                | Memory-map a contiguous `float16` array (≈6 kB/token) or quantize to 8-bit with FAISS.                                                                                                                                                                                                                                                                                                                                                                                                               | Blas-friendly, still differentiable if you keep a float32 copy for back-prop. |

---

### 2.  Embedding model shopping list (2025-Q2 snapshot)

| Model                                                      | Dim        | Order sensitivity       | MTEB-STS (en) | Licence / size    | Cynical verdict                                            |
| ---------------------------------------------------------- | ---------- | ----------------------- | ------------- | ----------------- | ---------------------------------------------------------- |
| **KaLM-Embedding-mini-instruct-v1.5** ([arXiv][2])         | 768 / 1024 | Moderate (EOS pooling)  | 83.4          | Apache-2.0, 0.5 B | Best *open* bang-for-buck; trainable on one A100.          |
| **E5-base-v2** ([Hugging Face][1])                         | 768        | Good (RoPE, EOS)        | 82.2          | Apache-2.0, 110 M | Smaller, easily fine-tuned token-wise.                     |
| **BGE-M3-large** (Beijing Gen-Embeddings) ([RAG News][3])  | 1024       | Mild                    | 84.0          | MIT, 400 M        | Solid, but Chinese-centric; English benchmarks flatter it. |
| **Snowflake-Arctic-embed-m** ([Snowflake][4])              | 768        | Moderate                | 84.5          | BSD-3, 180 M      | Nice if you already run on Snowflake infra.                |
| **OpenAI text-embedding-3-large (TE-3-L)** ([RAG News][3]) | 3072       | **Poor** (mean pooling) | 83-85\*       | Closed, API only  | Huge dim ≠ better semantics; order-blind.                  |

*Numbers vary by eval set; take them with scepticism.*

**Recommendation:** Fine-tune **E5-base-v2** (or KaLM) with a *token-level* head (see next section).  They’re small enough for quick iteration and already preserve some positional info.

---

### 3.  Two-stage controller architecture

```text
           target vector (sentence-level)
                    │
            ┌───────┴─────────┐
            │ Selector (Bi-enc)│  – predicts embedding of *word type*
            └───────┬─────────┘
                    │ intention vector
             FAISS nearest-k → candidate tokens
                    │
            ┌───────┴─────────┐
            │ Placer (Tiny Tr) │  – predicts position, may reorder
            └───────┬─────────┘
                    │ chosen (token, position)
         token buffer & hierarchical summary update
```

*Training:*

1. For each gold sentence **S** = w₁…w\_n
2. Teacher-forces the Selector with `emb(w_t)` and the Placer with *absolute position t*.
3. Minimise (i) cosine loss on predicted vs gold `emb(w_t)`; (ii) cross-entropy on position; (iii) a distance-to-target hinge loss after each step (encourages few-token completions).

This decouples *semantic choice* from *syntax/ordering*, side-steps the mixed-signal problem you anticipated.

---

### 4.  Buffer & summary engineering

* **Max tokens:** keep ≤ 32-40.  Once `len(buffer) == max_len`, roll the oldest 8 tokens into level-2 summary (`mean(pool(E_old))`) and keep going.  Your hierarchical vector stack then has ≤ \~6 items – small enough that dot-product distance remains informative.
* **Incremental updates:** update the running mean in **O(1)** without recomputing from scratch:
  `v' = (v * n + emb(new)) / (n+1)` (remember to renormalise).
* **Compression:** store cached token vectors in 8-bit (FAISS PQ) – negligible drop (<0.2 STS) but 4× RAM saving.

---

### 5.  Training data & negative mining

* Use any large English corpus (Wiki-40B, C4).
* Build **contrastive pairs**: `(prefix_i, sentence_i)` positive; `(prefix_i, sentence_j)` (j≠i) hard negatives.
* Add **PAWS-Wiki/QQP** for order-stress .
* Mine *false friends*: restrict levenshtein distance ≤ 2 but semantic distance > 0.6 – forces the selector to disambiguate homophones/homographs.

---

### 6.  Alternative “semantic-switch” sandboxes

Want a simpler demo of “vector-controlled switches” before full text generation?

| Environment             | Vector space       | Action set      | Why easier                                     |
| ----------------------- | ------------------ | --------------- | ---------------------------------------------- |
| **Color-mixing shader** | CIELab (3-d)       | Set RGB triplet | Small, smooth space; visual feedback.          |
| **Robot arm (2-DoF)**   | End-effector (x,y) | Motor pulses    | Continuous control, few “tokens”.              |
| **Music chords**        | 12-D pitch-class   | Play chord      | Discrete but small vocab; ordering irrelevant. |

---

### 7.  Opinionated closing thoughts

* **Cynic’s take:** single-vector text control is fashionable but inherently leaky – language is compositional, and compressing the history into 3 k-floats is wish-thinking.  Your hierarchical trick plus token-level supervision is the minimum to keep semantics intact.
* **Opposite view:** others argue you should abandon dense retrieval entirely and train a small decoder-only LLM with *direct* logit-space control (a la OpenAI’s “direct preference optimisation”).  It avoids the indirection of FAISS search, but you lose interpretability and the “switch” analogy.
* **My bias:** I’ve spent too long in vector search land; I may overweight its elegance versus raw end-to-end models.  Treat my enthusiasm accordingly.

---

### 8.  Next steps for you

1. Pick **E5-base-v2** + token-head, plug into FAISS, and try the two-stage controller on a 10 k-sentence subset.
2. Instrument *distance-to-target vs step*; you should see exponential convergence for easy sentences.
3. If order errors dominate, up-weight the Placer loss.
4. Ping back with plots; we can tune the curriculum or explore RL fine-tuning.

*If anything here is off or you need deeper math, let me know – happy to sharpen further.*

[1]: https://huggingface.co/intfloat/e5-base-v2?utm_source=chatgpt.com "intfloat/e5-base-v2 - Hugging Face"
[2]: https://arxiv.org/html/2501.01028v1 "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
[3]: https://ragaboutit.com/top-ai-embedding-models-in-2024-a-comprehensive-comparison/ "Top AI Embedding Models in 2024: A Comprehensive Comparison – News from generation RAG"
[4]: https://docs.snowflake.com/en/release-notes/2024/other/2024-05-08-embed-text-model?utm_source=chatgpt.com "May 08, 2024 — New model for vector embedding — Preview"

---

# Response 2

**Key items you now have**

1. **`vector_controlled_generator.py`** (open in the canvas to the right)

   * Contains:\*
   * `TokenEmbeddingModel` wrapping **E5-base-v2** with a learnable projection head → word-level vectors
   * `TwoStageController` that predicts

     * an *intention* (token-semantic) vector
     * logits over *insert-position* 0…*len(buffer)*
   * `VectorControlledGenerator`, wired to FAISS, maintaining a hierarchical summary and applying the two-stage policy in `step()`
   * A minimal fine-tuning stub to get you started

2. **Fine-tuning guidance**

| Question                  | Short answer                                                                                                                                                                                                                           |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| *Do I need to fine-tune?* | **Yes.**  The projection head is randomly initialised; without a few thousand update steps it will emit near-random intention vectors.  Freeze the E5 backbone at first, then optionally unfreeze top 4–6 layers if you hit a ceiling. |
| Dataset size              | 30 k – 100 k sentence triples `(target_vec, summary_vec, gold_token, gold_pos)` is enough for proof-of-concept.                                                                                                                        |
| Losses                    | Cosine loss on token vector + cross-entropy on position (see stub).                                                                                                                                                                    |
| Optimiser / sched         | AdamW, LR 1e-4 warm-up 1 k steps → cosine decay works fine.                                                                                                                                                                            |
| Hardware                  | One A100 or 3090 is plenty (controller ≈5 M params, frozen backbone).                                                                                                                                                                  |

3. **Next steps**

4. Build the *token bank*: feed each unique word from your training corpus through `learn_token()`.

5. Train `TwoStageController` with teacher forcing (see stub).

6. Drive the generator:

```python
tgt = embedder(['some target sentence']).to(device)
done = False
while not done:
    done = gen.step(tgt)
print(gen.empty_token_buffer())
```

4. Instrument distance-to-target after each step; adjust `min_sim` and the roll-up span to keep convergence under \~25 tokens.

**Uncertainties & biases**

* I froze the E5 backbone by default—faster but may cap ceiling performance.
* Placer head assumes ≤32-token buffers; raise `max_buffer_len` if you routinely exceed that.
* I skipped the cross-encoder rerank stage to keep the code compact; if retrieval errors dominate, add it.

**Happy hacking—let me know where you get stuck, and we’ll iterate.**

---

# Response 3

Below are three incremental upgrades you can layer on top of the MLP-head recipe that will usually give another +1 → +3 points of “local-neighbour quality” without exploding training time.

---

## 1 Hard-negative mining (in-batch, no extra data passes)

### Rationale

Your current InfoNCE sees only *easy* negatives (random other words in the mini-batch).
The head stops improving once it can separate **cat** from **xylophone**, even if **cat–tiger** is still too close.

### Minimal implementation

```python
# inside the training loop, just before computing logits
with torch.no_grad():
    # cosine similarity matrix (B×B) on the **current** batch
    sim = embeds @ embeds.T
    # for each anchor i, pick the hardest j  (highest sim but not i)
    hard_idx = sim.fill_diagonal_(-1).argmax(dim=-1)      # (B,)
hard_neg = embeds[hard_idx]                               # (B,d)

# concatenate: [anchor | positive=itself | hard-neg]
triplet = torch.cat([embeds, embeds, hard_neg], dim=0)    # (3B,d)
labels  = torch.arange(len(embeds), device=device_t).repeat(3)

logits  = (triplet @ triplet.T) / temperature
loss    = F.cross_entropy(logits, labels) / accum_steps
```

*No second forward pass*—the hardest negatives are mined from the same batch, so cost is negligible.

### Tip

Keep `batch_size` moderate (≥256) so each anchor has enough candidates; otherwise hardest-neg=easy-neg again.

---

## 2 Curriculum / dynamic temperature τ

### Why

Early steps need a *high* temperature (soft distribution → stable gradients).
Later you want a *low* τ to push very similar words apart.

### One-liner schedule

```python
base_tau   = 0.2          # starting value
final_tau  = 0.05         # value in last epoch
total_iter = epochs * len(dl)

for global_step, (batch_tokens) in enumerate(dl_epoch_loop):
    # linear decay
    temperature = base_tau + (final_tau - base_tau) * (global_step / total_iter)
    logits = embeds @ embeds.T / temperature
```

A cosine decay (mirroring LR scheduler) works too; empirical difference is tiny.

---

## 3 Contrastive + regularisation mix

* **Orthogonality penalty** – encourage basis diversity in the projection head:

```python
W = model.proj[-1].weight        # last Linear layer (d×hidden)
gram = W @ W.T                   # (d×d)
off_diag = gram - torch.diag(torch.diag(gram))
loss_ortho = 1e-4 * off_diag.pow(2).mean()
total_loss = loss + loss_ortho
```

* **Dropout inside the head** (e.g. after GELU) + `model.train()` keeps it active.

---

## Putting it together (patch sketch)

```python
hard_negatives = True
base_tau, final_tau = 0.2, 0.05

total_iter = epochs * len(dl)
for ep in range(epochs):
    for step, batch_tokens in enumerate(dl):
        it = ep * len(dl) + step
        tau = base_tau + (final_tau - base_tau) * it / total_iter

        z = model(batch_tokens)                      # (B,d), already L2-normed

        if hard_negatives and len(z) > 1:
            with torch.no_grad():
                sim = z @ z.T
                sim.fill_diagonal_(-1)
                hard_idx = sim.argmax(dim=-1)
            z_all = torch.cat([z, z, z[hard_idx]], dim=0)   # 3B
        else:
            z_all = z
        labels = torch.arange(len(z_all), device=device_t) % len(z)

        logits = z_all @ z_all.T / tau
        loss   = F.cross_entropy(logits, labels) / accum_steps

        # + orthogonality reg
        W = model.proj[-1].weight
        offd = (W @ W.T) - torch.diag(torch.diag(W @ W.T))
        loss += 1e-4 * offd.pow(2).mean() / accum_steps

        loss.backward()
        ...
```

---

## What kind of gain to expect?

| Metric                    | Linear head (τ = 0.1)           | MLP + hard-neg + τ-decay                                                                  |
| ------------------------- | ------------------------------- | ----------------------------------------------------------------------------------------- |
| Synonym AUROC (same test) | \~0.89                          | **0.91–0.93**                                                                             |
| Nearest-neighbour quality | animals & verbs sometimes fuzzy | noticeably crisper groups (*cat → kitty, cats, puppies…*; *run → running, flee, sprint…*) |

*Exact numbers depend on seed and LR; run 2–3 seeds to confirm.*

---

## Extra knobs (only if you still need more)

1. **Unfreeze top-4 E5 layers** after epoch 10, drop LR to `1e-5`.
   *Usually gives +0.01 AUROC and sharper rare-word embeddings.*

2. **Masked-token auxiliary loss**
   Add a `BCEWithLogits` that tries to predict the masked word from the CLS after passing the anchor through the frozen head.  Marginal win, code heavier.

3. **Sub-word pooling head**
   Average all token embeddings of the word (use tokenizer offsets) instead of CLS; helps with morphology but doubles encoder compute.

---

### Bottom line

* Your current **MLP-head + long warm-up** is already good.
* Hard-negatives + τ-decay give the biggest “easy win” without touching the backbone.
* Orthogonality regularisation is a low-cost extra.

Try the 20-line patch above; compare AUROC and eyeball neighbours again.
Ping me with the new scores or if anything is unclear.

---

# Response 4

**Mostly yes, but with a few caveats worth spelling out.**

| Component                           | What it really captures                                                                                                                                                                                                                                            | Why it helps your pipeline                                                                                                                                                             | Practical caveats                                                                                                                                                                                                    |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **E5-base-v2 backbone (frozen)**    | *Sentence-level* representation with **weak order awareness**. <br>– Uses RoPE positional encoding and the CLS token <br>– Swapped constituents ( “dog bites man” vs “man bites dog” ) still move the CLS vector, but not as dramatically as in a decoder-only LM. | *Summary vector*: if you L2-average a running buffer or keep a hierarchical stack, the backbone can still detect coarse ordering cues (e.g. subject → verb → object) and topical flow. | 1. Attention is bidirectional but **compresses everything into 768 dims** → subtle syntax vanishes for long buffers. <br>2. As a **bi-encoder**, it’s insensitive to word repetition counts (bag-of-fragments bias). |
| **Two-layer token head (your MLP)** | *Word-level* embedding that inherits sentence priors from E5 but sharpens local neighbourhoods via contrastive fine-tuning.                                                                                                                                        | *Token search / FAISS step*: intention vector lands closer to the right word; hard-neg mining makes near-synonyms distinguishable.                                                     | Head sees each word in isolation (“passage: cat”) so its notion of **part-of-speech** and **morphology** comes solely from E5; very rare homographs (“lead” metal vs verb) remain conflated.                         |

### Why this pairing works for your two-stage generator

1. **Forward direction (text → summary)**
   *As you buffer tokens*, the summary vector is produced by the frozen backbone; its mild order sensitivity helps the controller gauge “how far am I from the target sentence’s structure?” rather than just its bag-of-words content.

2. **Reverse direction (summary + intention → text)**
   *When the controller predicts an intention vector* the MLP head gives you a **sharply localised point** in the embedding space, so FAISS can map it to an actual surface word with high precision.

   Because both summary and token vectors ultimately live in *the **same E5 latent space*** (one aggregated, one local), the cosine distance between “current summary ⊕ candidate token” and “target sentence vector” is meaningful.  That alignment is what lets your generator converge toward the target.

### Where the design still leaks

| Limitation                                                                                                                           | Consequence                                                                                      | Possible patch                                                                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Sentence-level order ≠ full syntax.**  CLS can’t tell *“Alice gave Bob a book”* from passive voice variants when buffers are long. | The controller might choose the right lexical set but mis-order phrases.                         | Fine-tune the backbone (top 4 layers) with a combined sentence-level loss that punishes word swaps (PAWS-Wiki, ROCStories order dataset). |
| **Token head ignores multi-word expressions.**                                                                                       | Fails on fixed phrases (*“New York”*, *“United States”*).                                        | Extend `learn_token()` to add n-gram entries (FAISS can index “new\_york”).                                                               |
| **Homograph conflation.**                                                                                                            | “bass” (instrument) vs “bass” (fish) share one vector → contextless retrieval picks wrong sense. | Add a sense-disambiguation reranker or segment vocab by POS tags.                                                                         |

### Bottom line

* **Yes:** the *sentence-aware backbone* + *sharpened token head* is exactly what you need for (i) building a running summary and (ii) walking that summary back to the target sentence via vector search.
* **But:** order cues are good, not perfect; edge-cases around multi-word units and homographs will still need either controller fine-tuning or a lightweight reranker.

If you hit those edge-cases in practice, we can layer one of the patches above without redesigning the architecture.
