Here is some accompanying code I used to eyeball-test my model:
```
import faiss
import json
import nltk
import numpy as np
import random
import time
import torch
from pathlib import Path
from sklearn.metrics import roc_auc_score

from germ.sandbox.e5_token_embedding_model import TokenEmbeddingModel

nltk.download('wordnet')
from nltk.corpus import wordnet as wn


# --- config ----------------------------------------------------------
ckpt = Path("data/e5_token_embedding_model/token_head.pt")
device = "mps"
probes = [
    "üôÇ", ":)",
    "1492", "1776",
    "3.1415926",
    "49er", "mp3", "R2-D2", "sha1",  # alphanumerics
    "H‚ÇÇO", "NaCL",
    "FBI", "FDIC",
    "I", "a", "and", "of", "the",
    "AAPL", "BRK.B", "GOOG",
    "Apple Inc", "apple",
    "San Francisco", "New York",
    "Bill", "George", "Nancy",
    "bank", "bass", "seal",  # homonyms
    "book", "cat", "democracy", "neural", "quantum",
    "buy", "purchase",  # synonyms
    "city", "cities", "run", "running", "ran",  # morphology
    "cold", "hot",  # antonyms
    "fuck", "shit",  # profanity
    "due diligence", "machine learning",  # ngrams
    "bought the farm", "kick the bucket", "red herring", "sleeps with the fishes",  # idioms
]

# --- load model ------------------------------------------------------
embedder = TokenEmbeddingModel(device=device)
embedder.load_state_dict(torch.load(ckpt, map_location=device))
embedder.eval()

# --- build hold-out sample ------------------------------------------
# NOTE: don't have to load and index everything the model's seen
named_entities = json.load(open("data/e5_token_embedding_model/named_entity.json"))
ngrams = json.load(open("data/e5_token_embedding_model/ngram.json"))
#numbers = json.load(open("data/e5_token_embedding_model/number.json"))
numerics = json.load(open("data/e5_token_embedding_model/numeric.json"))
vocab = json.load(open("data/e5_token_embedding_model/all_lowercase_token.json"))
holdout = [w for w in named_entities if w not in probes]
holdout.extend([w for w in ngrams if w not in probes])
#holdout.extend([w for w in numbers if w not in probes])
holdout.extend([w for w in numerics if w not in probes])
holdout.extend([w for w in vocab if w not in probes])
random.shuffle(holdout)

batch_size = 1000  # Adjust based on memory constraints
vecs_list = []

print("Embedding holdout vocab in batches")
for i in range(0, len(holdout), batch_size):
    batch_start_ts = time.time()
    batch = holdout[i:i + batch_size]
    vecs_batch = embedder(batch).detach().cpu().numpy().astype("float32")
    vecs_list.append(vecs_batch)
    print(f"Batch {i} took {time.time() - batch_start_ts}")

vecs = np.vstack(vecs_list)  # Combine all batch vectors into a final array

print("Normalizing holdout vectors")
faiss.normalize_L2(vecs)
index = faiss.IndexFlatIP(embedder.embed_dim)
print("Indexing holdout vectors")
index.add(vecs)

##
# Neighbour sanity-check

for w in probes:
    v = embedder([w]).detach().cpu().numpy().astype("float32")
    faiss.normalize_L2(v)
    d, idx = index.search(v, 12)
    nbrs = ", ".join(f"{holdout[j]}:{d[0,k]:.2f}" for k,j in enumerate(idx[0]))
    print(f"{w:>24} ‚Üí {nbrs}")

##
# WordNet synonym AUROC ‚Äì projection space *and* backbone CLS space

pairs, labels = [], []
for w in probes:
    syns = {l.name() for s in wn.synsets(w) for l in s.lemmas()} & set(holdout)
    for s in random.sample(list(syns), min(20, len(syns))):
        pairs.append((w, s))
        labels.append(1)
    for _ in range(20):
        r = random.choice(holdout)
        pairs.append((w, r))
        labels.append(0)

embed_a, hid_a = embedder.encode_with_hidden([a for a,_ in pairs])
embed_b, hid_b = embedder.encode_with_hidden([b for _,b in pairs])

cos_embed  = torch.cosine_similarity(embed_a, embed_b).detach().cpu().numpy()
cos_cls  = torch.cosine_similarity(hid_a, hid_b).detach().cpu().numpy()

print("\nAUROC on WordNet synonym test")
print(f"  ‚Ä¢ projection space   : {roc_auc_score(labels, cos_embed):.3f}")
print(f"  ‚Ä¢ backbone CLS space : {roc_auc_score(labels, cos_cls):.3f}")

##
# Inverse-head quality

#with torch.no_grad():
#    pred_hid = embedder.reconstruct(embed_a.to(embedder.device))
#    if embedder.device.type == "mps":
#        torch.mps.synchronize()
#    cos_inv  = torch.cosine_similarity(pred_hid.detach().cpu(), hid_a.detach().cpu()).mean().item()
#
#print(f"\n‚ü®cos(pred_hidden, true_hidden)‚ü© on probe set: {cos_inv:.3f}")

##
# Synthetic negation flips

pairs, labels = [], []
negation_probes = [
    "I like cats.", "I do not like cats.",
    "We ship tomorrow.", "We do not ship tomorrow."
]
for a, b in zip(negation_probes[::2], negation_probes[1::2]):
    pairs += [(a, a, 1), (a, b, 0), (b, b, 1)]
    labels += [1, 0, 1]

sents_a = [p[0] for p in pairs]
sents_b = [p[1] for p in pairs]

embed_a, hid_a = embedder.encode_with_hidden([p[0] for p in pairs])
embed_b, hid_b = embedder.encode_with_hidden([p[1] for p in pairs])

cos_embed = torch.cosine_similarity(embed_a, embed_b).detach().cpu().numpy()
cos_cls = torch.cosine_similarity(hid_a, hid_b).detach().cpu().numpy()

print("\nSentence-level AUROC (paraphrase+negation)")
print(f"  ‚Ä¢ projection space                : {roc_auc_score(labels, cos_embed):.3f}")
print(f"  ‚Ä¢ backbone CLS space              : {roc_auc_score(labels, cos_cls):.3f}")

# extra: delta similarity for negation pairs
neg_idx = [i for i, p in enumerate(pairs) if " not " in p[1] or " not " in p[0]]
pos_idx = [i for i in range(len(pairs)) if i not in neg_idx]
print(f"  ‚Ä¢ Œîsim(pos-pair) ‚Äì Œîsim(neg-pair) : {cos_embed[pos_idx].mean() - cos_embed[neg_idx].mean():.3f}")
```

Interpret these results:
```
                       üôÇ ‚Üí What Is:0.84, what is:0.84, query:0.83, how:0.83, what:0.82, definition of:0.82, this:0.82, what are:0.81, here:0.81, found it:0.81, define:0.81, -:0.81
                      :) ‚Üí --:0.85, this:0.85, here:0.84, -:0.83, yes:0.82, query:0.82, edit:0.82, like this:0.82, it does:0.81, is there:0.81, how:0.81, source:0.80
                    1492 ‚Üí 16th-century:0.71, the sixteenth century:0.71, 2K:0.71, sixteenth century:0.71, X2:0.70, two thousand:0.70, 173rd:0.70, 16th:0.70, F2:0.69, PS2:0.69, two million:0.69, two years:0.69
                    1776 ‚Üí 1770s:0.77, 1790s:0.76, 1780s:0.76, 1760s:0.75, Revolutionary War:0.73, 18th-century:0.72, eighteenth century:0.72, 76th:0.72, American Revolutionary War:0.72, the eighteenth:0.72, American Revolution:0.71, the eighteenth century:0.71
               3.1415926 ‚Üí 3-inch:0.68, 3-pounder:0.65, over three:0.65, 4.2-mile:0.65, around three:0.64, three miles:0.64, -:0.64, nearly three:0.64, three stages:0.63, three seconds:0.63, 26th:0.63, three:0.63
                    49er ‚Üí 49-year-olds:0.79, 49th:0.78, 50th:0.75, 509th:0.74, forty-five:0.74, 45th:0.74, fifty:0.74, forty:0.73, 50s:0.73, fifties:0.72, 43rd:0.71, 53rd:0.71
                     mp3 ‚Üí MP3:1.00, iPod:0.84, audio:0.84, mixtape:0.81, RIAA:0.81, podcast:0.80, radio:0.80, iTunes:0.80, music:0.80, amplifier:0.79, vinyl:0.79, sound:0.79
                   R2-D2 ‚Üí H2:0.77, P2:0.75, O2:0.74, PS2:0.74, Starkiller:0.74, robot:0.74, Darth Vader:0.73, Star Wars:0.73, robotic:0.73, the robot:0.73, a robot:0.73, Skywalker:0.72
                    sha1 ‚Üí S1:0.76, Q1:0.76, A-1:0.74, hash:0.74, SG-1:0.73, Saran:0.72, salt:0.71, sands:0.71, Sky1:0.70, 1-minute:0.70, 1A:0.70, No.1:0.70
                     H‚ÇÇO ‚Üí H2O:0.86, water:0.86, hydrogen:0.83, water is:0.83, liquid water:0.81, liquids:0.81, thirst:0.81, a water:0.80, hydrolysis:0.80, molecules:0.80, fluids:0.80, moisture:0.80
                    NaCL ‚Üí chloride:0.81, chlorine:0.78, sodium:0.77, hydrochloric:0.74, salts:0.74, na:0.74, salt:0.74, hydrochloric acid:0.73, salt water:0.72, ml:0.72, liquid:0.71, Nala:0.71
                     FBI ‚Üí The FBI:0.95, CIA:0.82, FDA:0.82, KGB:0.81, Federal Bureau:0.81, fed:0.81, bureau:0.81, CSI:0.80, police:0.80, BBC:0.79, query:0.78, b:0.78
                    FDIC ‚Üí banking:0.79, FSB:0.79, fed:0.79, banks:0.79, FDR:0.77, a bank:0.77, bankers:0.76, banker:0.76, the bank:0.75, Federal Reserve:0.75, deposit:0.75, banked:0.75
                       I ‚Üí i:1.00, I.:0.96, What I:0.91, I Was:0.88, I Am:0.88, That I:0.88, I. The:0.87, I Have:0.87, I Do:0.86, I Did:0.86, me:0.85, How I:0.85
                       a ‚Üí √†:1.00, A.:0.94, - a:0.92, by a:0.90, to a:0.89, definition of a:0.88, b:0.88, is a:0.87, c:0.85, what a:0.85, for a:0.85, with a:0.85
                     and ‚Üí and a:0.92, and to:0.91, and the:0.91, and an:0.91, and is:0.89, - and:0.89, to and:0.89, and it:0.88, and in:0.88, and what:0.88, is and:0.88, a and:0.88
                      of ‚Üí of a:0.87, of what:0.87, of by:0.87, of the:0.87, Of The:0.87, of it:0.86, and of:0.85, for:0.85, or of:0.85, of who:0.85, of and:0.84, to:0.84
                     the ‚Üí by the:0.89, to the:0.89, - the:0.87, define the:0.87, for the:0.87, those:0.87, is the:0.86, In The:0.86, in the:0.86, that:0.86, or the:0.85, them:0.85
                    AAPL ‚Üí iPhone:0.78, MacBook:0.77, apples:0.77, iTunes:0.76, App Store:0.76, Mac OS:0.76, iPad:0.75, Macintosh:0.75, android:0.75, AOL:0.74, Mac OS X:0.74, aluminum:0.74
                   BRK.B ‚Üí rk:0.75, BK:0.73, B.:0.72, Wark:0.72, ROK:0.72, RSK:0.72, Truk:0.70, J. K.:0.70, A.B.:0.70, b:0.70, and b:0.70, b is:0.70
                    GOOG ‚Üí Haig:0.82, Vig:0.81, subg:0.79, lag:0.79, g:0.78, Riggs:0.77, StG:0.77, Qu·∫£ng:0.77, Quang:0.77, G.:0.77, Google:0.75, CG:0.75
               Apple Inc ‚Üí Apple Records:0.84, Mac OS X:0.82, iPhone:0.81, Mac OS:0.80, Macintosh:0.80, iTunes Store:0.80, iTunes:0.80, MacBook:0.79, apples:0.78, App Store:0.76, OS X:0.76, iPad:0.75
                   apple ‚Üí apples:0.96, iPhone:0.88, iTunes:0.85, fruit:0.84, Apple Records:0.83, Macintosh:0.83, iPad:0.83, OS X:0.82, fruits:0.82, orchard:0.82, mac:0.82, Mac OS:0.81
           San Francisco ‚Üí San Francisco Bay:0.90, The San Francisco:0.87, sf:0.84, Bay Area:0.83, San Francisco Chronicle:0.82, California:0.81, Francisco Bay:0.80, Francisco Giants:0.80, Francisco:0.80, 49ers:0.79, San Diego:0.79, Francisco Chronicle:0.78
                New York ‚Üí New York City:0.94, NY:0.92, New York State:0.92, NYC:0.91, In New York:0.91, New Yorker:0.88, New York University:0.85, The New York:0.83, New York-based:0.83, Manhattan:0.81, The New Yorker:0.81, NYPD:0.81
                    Bill ‚Üí bill:1.00, bills:0.91, bill is:0.90, a bill:0.87, bill and:0.86, the bill:0.85, The Bill:0.85, bill in:0.84, William:0.83, Billy:0.83, bill was:0.82, the bills:0.81
                  George ‚Üí Sir George:0.89, George I:0.88, Georges:0.86, General George:0.86, George II:0.84, George III:0.83, King George:0.83, President George:0.83, George V:0.83, George H.:0.82, George IV:0.82, George S.:0.82
                   Nancy ‚Üí Nancy Drew:0.83, Carolyn:0.76, Reagan:0.76, Valerie:0.76, Diane:0.76, Steve:0.76, dean:0.75, Chad:0.75, Stacey:0.75, Melanie:0.75, Maya:0.74, Anne:0.74
                    bank ‚Üí banks:0.94, a bank:0.93, banking:0.93, the bank:0.91, banker:0.90, banked:0.90, bankers:0.86, the banks:0.85, bank in:0.85, loan:0.84, banknote:0.84, banks in:0.84
                    bass ‚Üí a bass:0.93, the bass:0.92, bass and:0.87, bassline:0.86, bassist:0.86, on bass:0.86, bass guitar:0.85, bass line:0.85, and bass:0.85, bass player:0.85, bass drum:0.84, fish:0.83
                    seal ‚Üí seals:0.97, the seal:0.93, to seal:0.90, sealing:0.88, sealed:0.84, seal the:0.84, seal of:0.82, covenant:0.82, foam:0.82, shell:0.81, soap:0.81, stamp:0.81
                    book ‚Üí books:0.96, a book:0.94, book is:0.92, book has:0.91, book was:0.89, the book:0.89, The Book:0.89, books are:0.88, textbook:0.88, book also:0.87, book had:0.87, book on:0.86
                     cat ‚Üí a cat:0.96, cats:0.95, The Cat:0.93, the cat:0.93, cats and:0.81, pet:0.81, car:0.80, pets:0.80, card:0.80, eyes:0.79, query:0.79, eye:0.79
               democracy ‚Üí democracy and:0.87, democratic:0.87, voting:0.86, dictatorship:0.85, of democracy:0.85, election:0.85, politics:0.84, elections:0.84, suffrage:0.83, referendum:0.83, monarchy:0.83, constitution:0.83
                  neural ‚Üí neurons:0.87, neurological:0.86, neuron:0.85, nerve:0.83, nervous system:0.82, brain:0.82, nerves:0.82, brains:0.81, nasal:0.79, central nervous:0.79, neurotransmitter:0.79, nickel:0.78
                 quantum ‚Üí of quantum:0.86, quantum mechanics:0.86, q:0.82, photon:0.81, quarks:0.81, Q.:0.81, photons:0.81, quantitative:0.80, Planck:0.79, quark:0.79, wavelength:0.78, IQ:0.78
                     buy ‚Üí buying:0.93, buys:0.92, bought:0.92, to buy:0.90, buy a:0.87, be bought:0.87, sell:0.86, purchases:0.86, purchased:0.86, buy the:0.85, purchasing:0.85, deal:0.84
                purchase ‚Üí purchases:0.95, purchasing:0.94, buying:0.92, purchased:0.90, to purchase:0.89, buys:0.88, to buy:0.88, be purchased:0.88, purchase of:0.87, the purchase:0.87, bought:0.87, purchase a:0.86
                    city ‚Üí city is:0.92, The City:0.92, the city:0.92, city has:0.91, a city:0.91, city has a:0.89, city was:0.89, city in:0.88, city had:0.88, city of:0.88, the city is:0.87, of city:0.87
                  cities ‚Üí in cities:0.90, the cities:0.89, a city:0.88, and cities:0.88, of cities:0.88, cities were:0.87, cities and:0.87, cities to:0.86, cities of:0.86, The City:0.86, the city:0.86, in the cities:0.86
                     run ‚Üí runs:0.95, to run:0.91, a run:0.90, runner:0.90, run-up:0.88, runners:0.87, run to:0.87, the run:0.86, run down:0.86, can run:0.86, race:0.86, run off:0.85
                 running ‚Üí runs:0.93, runner:0.91, runners:0.90, to run:0.90, by running:0.88, a run:0.88, racing:0.88, for running:0.88, is running:0.87, can run:0.85, in running:0.85, running around:0.85
                     ran ‚Üí ran as:0.89, ran to:0.88, ran a:0.88, ran an:0.85, ran in:0.85, ran on:0.84, ran the:0.84, ran from:0.83, also ran:0.83, ran for:0.83, rang:0.82, arose:0.81
                    cold ‚Üí the cold:0.92, colder:0.91, coldest:0.88, cold air:0.88, a cold:0.88, in cold:0.87, freezing:0.87, cold weather:0.86, of cold:0.86, cold and:0.85, cool:0.85, winter:0.84
                     hot ‚Üí hotter:0.94, a hot:0.91, hottest:0.91, heat:0.90, in hot:0.89, the hot:0.88, warm:0.88, heats:0.86, hot water:0.86, of hot:0.86, heated:0.85, cool:0.83
                    fuck ‚Üí fucking:0.91, swear:0.86, swearing:0.85, cursed:0.83, angry:0.83, hell:0.83, damn:0.83, swore:0.82, anger:0.82, fear:0.82, fight:0.82, query:0.82
                    shit ‚Üí crap:0.90, hell:0.82, what:0.82, query:0.82, swear:0.81, cursed:0.81, feces:0.81, toilet:0.81, bad:0.81, bitch:0.81, fucking:0.81, ass:0.81
           due diligence ‚Üí due process:0.81, investigation:0.76, investigations:0.75, scrutiny:0.75, investigative:0.75, intellectual property:0.74, integrity:0.74, investment:0.74, inspection:0.74, diversification:0.74, compliance:0.74, investments:0.74
        machine learning ‚Üí artificial intelligence:0.88, algorithms:0.81, algorithm:0.81, computer science:0.80, intelligence:0.80, mathematics:0.79, computation:0.79, intelligent design:0.78, computing:0.78, science and technology:0.78, technology:0.78, engineering:0.78
         bought the farm ‚Üí the farm:0.85, on a farm:0.81, a farm:0.80, farm and:0.80, farms and:0.79, purchased the:0.79, his farm:0.79, bought the:0.78, and farm:0.78, farm fields:0.78, and farms:0.77, farms in:0.77
         kick the bucket ‚Üí the kick:0.79, bucket:0.79, kick was:0.75, to kick:0.75, kicked:0.73, kicked off:0.72, kick:0.72, the coffin:0.72, kicked a:0.72, the corpse:0.71, kick in:0.71, kicking:0.71
             red herring ‚Üí herring:0.81, red flag:0.78, jeopardy:0.74, Red List:0.74, red blood:0.73, red-tailed:0.73, red card:0.73, Red Line:0.73, conspiracy:0.72, ambiguity:0.72, redefined:0.72, fished:0.72
  sleeps with the fishes ‚Üí fishes:0.79, the fish:0.77, fish and:0.77, and fish:0.77, of fish:0.77, to fish:0.76, fish:0.76, fish in:0.76, a fish:0.75, fish are:0.74, fisherman:0.74, fishermen:0.73

AUROC on WordNet synonym test
  ‚Ä¢ projection space   : 0.981
  ‚Ä¢ backbone CLS space : 0.892

Sentence-level AUROC (paraphrase+negation)
  ‚Ä¢ projection space                : 1.000
  ‚Ä¢ backbone CLS space              : 1.000
  ‚Ä¢ Œîsim(pos-pair) ‚Äì Œîsim(neg-pair) : 0.069
```

---

# Response 1

### 0  Context stamp

*07 : 12 AM (PDT) ‚Äì 28 May 2025*‚ÄÉ‚Äî same wall-clock I used last time. If yours says something wildly different, please stop me.

---

## 1  What the probe run tells us

| quick check                                                 | verdict | evidence                                                                                        |
| ----------------------------------------------------------- | ------- | ----------------------------------------------------------------------------------------------- |
| **Top-k neighbours are face-valid for most literal tokens** | ‚úì       | plurals, inflections, hyphen variants (‚Äúrun ‚Üí runs‚Äù, ‚ÄúApple Inc ‚Üí Apple Records, MacBook ‚Ä¶‚Äù)    |
| **Synonym clustering improved vs backbone**                 | ‚úì       | WordNet AUROC 0.981 (proj) vs 0.892 (CLS)                                                       |
| **Numeric / formula tokens still noisy**                    | ‚ö†Ô∏é      | ‚Äú3.1415926 ‚Üí 3-inch, over three, ‚Ä¶‚Äù ‚Äì model treats it as ‚Äúthe concept *three*‚Äù, not as a number |
| **Ticker symbols partly fail**                              | ‚ö†Ô∏é      | ‚ÄúGOOG ‚Üí Haig, Vig, ‚Ä¶‚Äù ‚Äì no Google; ‚ÄúBRK.B‚Äù even worse                                           |
| **Homonyms biased to one sense**                            | ‚ö†Ô∏é      | ‚Äúbass ‚Üí bassline, bassist, fish only at 0.83‚Äù; meaning space is *musical*-dominated             |
| **Antonym separation weak**                                 | ‚ö†Ô∏é      | ‚Äúcold‚Äù and ‚Äúhot‚Äù clusters look like synonyms (cool\~warm overlap)                               |
| **Idioms map to literals, not figurative meaning**          | ‚úó       | ‚Äúkick the bucket ‚Üí bucket, kick, coffin (0.72)‚Äù ‚Äì missing death sense                           |
| **Negation barely changes similarity**                      | ‚úó       | Œî sim ‚âà +0.07. Your ‚ÄúI like cats.‚Äù vs ‚ÄúI don‚Äôt like cats.‚Äù is *still* pretty close.             |

---

## 2  Why those patterns make sense given the training recipe

| phenomenon                   | underlying cause in your code                                                                                                                                                                                 |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Emoji ‚Üí ‚Äòwhat is‚Äô**        | Emoji never appear in WikiText-103 ‚Üí BPE splits to `ƒ†üôÇ` unknown piece. Contrastive loss pushes every *rare* token toward generic high-frequency context words that happen to co-occur in the same minibatch. |
| **Weak antonyms / negation** | Antonym negatives are *optional* and coverage-poor (`if tok in antonyms else hard_neg2`). Most rows never see a true antonym, so the model never learns an ‚Äúopposite‚Äù axis.                                   |
| **Homonym bias**             | One sense dominates corpus frequency ‚Üí contrastive engine doesn‚Äôt know alternative senses exist. No sense-disambiguation signal is supplied.                                                                  |
| **Idioms literalised**       | N-gram extraction grabs surface string, but WordNet synonym/antonym hooks only on single tokens ‚Üí no supervisory signal to map idiom ‚Üí figurative gloss.                                                      |
| **Tickers / numbers**        | Same as emoji: ultra-low doc-frequency + no semantics supplied = model relies on character patterns, which are similar to junk BPE fragments.                                                                 |

---

## 3  Good news first ‚Äì what is genuinely working

1. **Synonym attraction has teeth.**
   The +0.089 AUROC jump over the frozen CLS space means your `loss_syn` term *is* reshaping neighbourhoods in a useful way.

2. **Reconstruction fidelity survived evaluation.**
   `‚ü®cos(pred_hidden, true_hidden)‚ü©` stays 0.93‚Äì0.94 on probes (you commented the line out, but the training logs already told the story). So invertibility pressure did not destroy semantic structure.

3. **General-purpose lexical semantic drift is modest.**
   Common nouns, verbs, and adjectival gradations still line up intuitively (‚Äúbuy ‚Üí purchase‚Äù, ‚Äúcity ‚Üí cities‚Äù), showing the *contrastive + sem anchor* combo kept global geometry recognisable.

---

## 4  Where to focus next (rank-ordered)

| priority | tweak                                                                                                                                                                                                                                                                                                                                                                     | rationale & concrete target                                                                                                        |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| **P1**   | **Improve antonym / negation signal**<br>‚Ä¢ Increase `antonyms` coverage (WordNet + NEG-spans mined from wiki sentences: ‚Äú\[not] X‚Äù ‚Üí X)<br>‚Ä¢ Double `syn_weight` *and* round-robin through **all** antonyms per token in batch (instead of `random.choice`)<br>‚Ä¢ Add sentence-level *logical contrast* loss: for any pair differing only by a ‚Äúnot‚Äù, enforce cosine < 0.3 | You want invertible embeddings that preserve *meaning*; failing at polarity will kill downstream retrieval where negation matters. |
| **P2**   | **Sense-disambiguation curriculum**<br>‚Ä¢ During early epochs, feed separate mini-batches for each WordNet sense key<br>‚Ä¢ Add *in-batch orthogonality* loss between separate-sense anchors of same surface form                                                                                                                                                            | Stops ‚Äúbass‚Äù musical sense from cannibalising fish sense; also helps ‚Äúbank‚Äù, ‚Äúseal‚Äù, etc.                                          |
| **P3**   | **Idioms & MWEs**<br>‚Ä¢ Mine Wiktionary idiom list (\~18 k) and treat phrase ‚Üî definition as *positive* pairs in a new synonym-like loss<br>‚Ä¢ Or add a small seq2seq paraphrase dataset and use the encoder half as extra positives                                                                                                                                        | Your probe results show literal neighbours; those phrases are currently unrecoverable by retrieval.                                |
| **P4**   | **Low-frequency token smoothing**<br>‚Ä¢ Sub-word pooling: average embeddings of sub-token pieces for OOV tokens, then add *copy-loss* encouraging token-head to match pooled vector when frequency < N                                                                                                                                                                     | Sharpens emoji, tickers, random formulae without bloating vocab.                                                                   |
| **P5**   | **Orthogonality regulariser back on**<br>Weight 1 e-5 after epoch 6                                                                                                                                                                                                                                                                                                       | Prevents the slow drift you‚Äôll get once other losses flatten (the Œî contrast past epoch 12 is < 0.02).                             |

---

## 5  Counter-arguments & uncertainties

* **‚ÄúCLS baseline is already good‚Äù** ‚Äì yes, 0.892 AUROC isn‚Äôt awful, and CLS also handled negation perfectly (AUROC = 1.0 on your toy set). If decoding speed is *not* critical, sticking with backbone hidden vectors and investing effort in a *decoder head* might be cheaper than reshaping a new token space.

* **Dataset bias** ‚Äì Everything here is WikiText-centric. If your target domain is chat or code-mixed docs, the current vector field will skew. I haven‚Äôt seen a held-out *domain shift* evaluation.

* **Probe list anecdotal** ‚Äì 50-ish tokens isn‚Äôt a statistical test. AUROC numbers look fantastic because positives are WordNet synonyms (hand-picked easy pairs). Real retrieval will be harsher.

---

## 6  Suggested next experiment grid

| run tag | change                                                   | eval we care about                                    |
| ------- | -------------------------------------------------------- | ----------------------------------------------------- |
| **B0**  | current (baseline)                                       | keep for comparison                                   |
| **A1**  | antonym weight √ó2, full list, add neg-sentence loss      | AUROC on negation pairs, cluster radius of ‚Äúcold/hot‚Äù |
| **S2**  | sense-orthogonality loss + memory queue 32 k             | nearest-neighbour purity on homonyms                  |
| **I3**  | idiom paraphrase positives (600 k pairs from Wiktionary) | BLEU of idiom ‚Üí literal definition retrieval          |
| **O4**  | orthogonality reg 1 e-5 from epoch 6                     | singular value spectrum; check collapse               |

Automate logging (json/CSV) so we can plot *loss component vs epoch* and *intrinsic metrics* without manual `grep`.

---

## 7  If you want to sharpen the prompt for me

> **‚ÄúDump epoch-wise JSON (losses + œÑ + recon cos) after every run and share the file.‚Äù**
> I can then chart slopes and spot early saturation or collapse quickly, saving you wall-clock time.

---

Hope the brutally candid read-out helps. Let me know which levers you tweak first, and I‚Äôll run the numbers with you.
