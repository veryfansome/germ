from openai import OpenAI
from pydantic import BaseModel
from typing import Optional
import json
import tiktoken

from bot.db import MessageReceived, MessageReplied, SessionLocal
from bot.logging_config import logging
from bot.vector_store import OpenAITextEmbedding3SmallDim1536

logger = logging.getLogger(__name__)

DEFAULT_CHAT_MODEL = "gpt-4o"
DEFAULT_IMAGE_MODEL = "dall-e-3"
ENABLED_TOOLS = {
    "generate_image": {
        "type": "function",
        "function": {
            "name": "generate_image",
            "description": "Generate an image from a textual prompt. Returns the image's URL string.",
            "parameters": {
                "type": "object",
                "properties": {
                    "prompt": {
                        "type": "string",
                        "description": "The textual prompt used to generate the image."
                    }
                },
                "required": ["prompt"]
            },
        },
        "callback": lambda url, arguments: f"[![{arguments['prompt']}]({url})]({url})"
    }
}


class ChatBookmark(BaseModel):
    id: Optional[int] = None
    is_test: Optional[bool] = True
    message_received_id: int
    message_replied_id: int
    message_replied_content: str


class ChatMessage(BaseModel):
    content: str
    is_test: Optional[bool] = True
    role: str


class ChatRequest(BaseModel):
    is_test: Optional[bool] = True
    messages: list[ChatMessage]
    system_message: Optional[str] = ""
    temperature: Optional[float] = 0.0


class LinkedMessageIds(BaseModel):
    is_test: Optional[bool] = True
    message_received_id: int
    message_replied_id: int


class OpenAIChatBot:

    def __init__(self, model=DEFAULT_CHAT_MODEL):
        self.model = model
        self.default_vector_store = OpenAITextEmbedding3SmallDim1536()  # TODO: Dynamically explore topical vector DBs?
        self.enabled_vector_stores = [self.default_vector_store]

    def chat(self, messages: list[ChatMessage],
             is_test=True,
             system_message=None,
             temperature: float = 0.0) -> object:
        new_chat_message: ChatMessage = messages[-1]
        logger.debug("received: %s", new_chat_message.content)

        ##
        # TODO: reinforcement learning based selection >>HERE<<
        #   - frequency_penalty, Optional, Defaults to 0,  Number between -2.0 and 2.0. Positive values penalize new
        #     tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat
        #     the same line verbatim.
        #   - logit_bias, Optional, Defaults to null, Accepts a JSON object that maps tokens (specified by their token
        #     ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to
        #     the logits generated by the model prior to sampling. The exact effect will vary per model, but values
        #     between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should
        #     result in a ban or exclusive selection of the relevant token.
        #   - presence_penalty, Optional, Defaults to 0, Number between -2.0 and 2.0. Positive values penalize new
        #     tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about
        #     new topics.
        #   - temperature, Optional, Defaults to 1, What sampling temperature to use, between 0 and 2. Higher values
        #     like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and
        #     deterministic. Generally, alter this or top_p but not both.
        #   - top_p, Optional, Defaults to 1, An alternative to sampling with temperature, called nucleus sampling,
        #     where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the
        #     tokens comprising the top 10% probability mass are considered. Generally, alter this or temperature but
        #     not both.
        #
        # Ref: https://platform.openai.com/docs/api-reference/chat/create

        # Trim message list to avoid hitting selected model's token limit.
        enc = tiktoken.encoding_for_model(self.model)
        total_tokens = len(enc.encode(system_message))
        reversed_chat_frame = []
        for chat_message in reversed(messages):  # In reverse because message list is ordered from oldest to newest.
            message_dict = chat_message.model_dump()
            message_tokens = len(enc.encode(message_dict['content']))
            # If adding `message_tokens` pushes us over the limit, stop appending
            if message_tokens + total_tokens > enc.max_token_value:
                break
            total_tokens += message_tokens
            reversed_chat_frame.append(message_dict)
        chat_frame = tuple(reversed(reversed_chat_frame))  # Undo previously reversed order

        # Update message history
        message_received = MessageReceived(
            chat_frame=json.dumps(chat_frame[:-1]).encode('utf-8'),
            content=new_chat_message.content.encode('utf-8'),
            is_test=is_test,
            role=new_chat_message.role,
        )
        with SessionLocal() as session:
            session.add(message_received)
            session.commit()
            session.refresh(message_received)

        # Do completion request
        response = OpenAI().chat.completions.create(
            messages=(([{"role": "system", "content": system_message}] if system_message else [])
                      + [x for x in chat_frame]),
            model=self.model,
            n=1,
            temperature=temperature,
            # A list of tools the model may call. Currently, only functions are supported as a tool. Use this to
            # provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
            tools=tool_selection_wrapper(),
            # Controls which (if any) tool is called by the model. none means the model will not call any tool and
            # instead generates a message. auto means the model can pick between generating a message or calling one or
            # more tools. required means the model must call one or more tools. Specifying a particular tool via
            # {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. `none` is
            # the default when no tools are present. `auto` is the default if tools are present.
            tool_choice="auto",
        )
        """
        $ curl -s localhost:8001/chat \
            -H 'content-type: application/json' \
            -X POST -d '{"messages": [{"role": "user", "content": "Hello"}]}'
        {
          "id": "chatcmpl-9XLdhn7SsScrsPtEa1SNg37BxcH4M",
          "choices": [
            {
              "finish_reason": "stop",
              "index": 0,
              "logprobs": null,
              "message": {
                "content": "Hello! How can I assist you today?",
                "role": "assistant",
                "function_call": null,
                "tool_calls": null
              }
            }
          ],
          "created": 1717735033,
          "model": "gpt-4-0613",
          "object": "chat.completion",
          "system_fingerprint": null,
          "usage": {
            "completion_tokens": 9,
            "prompt_tokens": 8,
            "total_tokens": 17
          }
        }
        """
        new_response = response.choices[0].message
        logger.debug("response: %s", new_response.content)

        # If a tool should be used, call it.
        if new_response.tool_calls:
            tool_args = json.loads(new_response.tool_calls[0].function.arguments)
            new_response.content = tool_wrapper(new_response.tool_calls[0].function.name, tool_args)

        ##
        # TODO: post-response >>HERE<<

        # Update message history
        message_replied = MessageReplied(
            content=new_response.content.encode('utf-8'),
            is_test=is_test,
            message_received_id=message_received.id,
        )
        with SessionLocal() as session:
            session.add(message_replied)
            session.commit()
            session.refresh(message_replied)
        return {
            "message_received_id": message_received.id,
            "message_replied_id":  message_replied.id,
            "response": response,
        }


def do_on_text(directive: str, text: str, model=DEFAULT_CHAT_MODEL, max_tokens=140, stop=None, temperature=0.0) -> str:
    response = OpenAI().chat.completions.create(
        messages=[{"role": "user", "content": f'{directive}: {text}'}],
        model=model,

        max_tokens=max_tokens, n=1, stop=stop, temperature=temperature,
    )
    return response.choices[0].message.content.strip()


def generate_image(prompt: str, model=DEFAULT_IMAGE_MODEL) -> object:
    response = OpenAI().images.generate(
        prompt=prompt,
        model=model,
        n=1,
    )
    return response.data[0].url


def tool_selection_wrapper() -> list:
    tools = []
    for tool_spec in ENABLED_TOOLS.values():
        new_spec = tool_spec.copy()
        new_spec.pop("callback")
        tools.append(new_spec)
    return tools


def tool_wrapper(tool_func: str, arguments: dict) -> str:
    logging.info("tool call: %s(%s)", tool_func, arguments)
    if "callback" in ENABLED_TOOLS[tool_func]:
        return ENABLED_TOOLS[tool_func]["callback"](
            globals()[tool_func](**arguments),
            arguments
        )
    else:
        return globals()[tool_func](**arguments)
