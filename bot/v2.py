from concurrent.futures import ThreadPoolExecutor
from typing_extensions import Literal
import os

from api.models import ChatMessage
from ml.message_categorizer import extract_message_features
from ml.model_selector import (ENABLED_MODELS,
                               ENABLED_TOOLS as MODEL_SELECTOR_TOOLS,
                               generate_embeddings, predict_model)
from utils.openai_utils import (DEFAULT_CHAT_MODEL, ChatCompletion,
                                handle_feedback, summarize_multiple_completions, trim_chat_frame)
from bot.v1 import chat as v1_chat
from db.models import MessageReplied, SessionLocal
from observability.logging import logging

executor = ThreadPoolExecutor(max_workers=os.getenv("MAX_EXECUTOR_WORKERS", 3))
logger = logging.getLogger(__name__)


class UserMessageSubscriber:
    def __init__(self):
        pass


# TODO: Use a real class?
subscribers = {
    "model_selector_trainer": {
        "message_handler": handle_feedback,
        "tools": MODEL_SELECTOR_TOOLS,
    }
}


def chat(messages: list[ChatMessage],
         system_message: str = None,
         temperature: float = 0.0,
         tools: dict[str, dict] = None,
         tool_choice: Literal['auto', 'none'] = 'none') -> object:
    new_chat_message: ChatMessage = messages[-1]
    logger.debug("received: %s", new_chat_message.content)

    # Start background tasks
    subscriber_task_results = {}
    for subscriber, attr in subscribers.items():
        subscriber_task_results[subscriber] = executor.submit(
            attr['message_handler'], messages, attr['tools'],)

    # Extract message features
    message_features = extract_message_features(
        trim_chat_frame(messages, DEFAULT_CHAT_MODEL, system_message=system_message))
    logger.info(message_features)

    # Generate embeddings and predict the best model
    embeddings = generate_embeddings(new_chat_message.content, message_features)
    model = ENABLED_MODELS[predict_model(embeddings)]
    logger.info("predicted model: %s", model)

    ##
    # TODO: reinforcement learning based selection >>HERE<<
    #   - frequency_penalty, Optional, Defaults to 0,  Number between -2.0 and 2.0. Positive values penalize new
    #     tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat
    #     the same line verbatim.
    #   - logit_bias, Optional, Defaults to null, Accepts a JSON object that maps tokens (specified by their token
    #     ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to
    #     the logits generated by the model prior to sampling. The exact effect will vary per model, but values
    #     between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should
    #     result in a ban or exclusive selection of the relevant token.
    #   - presence_penalty, Optional, Defaults to 0, Number between -2.0 and 2.0. Positive values penalize new
    #     tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about
    #     new topics.
    #   - temperature, Optional, Defaults to 1, What sampling temperature to use, between 0 and 2. Higher values
    #     like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and
    #     deterministic. Generally, alter this or top_p but not both.
    #   - top_p, Optional, Defaults to 1, An alternative to sampling with temperature, called nucleus sampling,
    #     where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the
    #     tokens comprising the top 10% probability mass are considered. Generally, alter this or temperature but
    #     not both.
    #
    # Ref: https://platform.openai.com/docs/api-reference/chat/create

    chat_response = v1_chat(messages,
                            model=model,
                            system_message=system_message,
                            temperature=temperature,
                            tools=tools,
                            tool_choice=tool_choice)

    replacement_candidates: list[ChatCompletion] = []
    for subscriber, future_result in subscriber_task_results.items():
        task_result_completion: ChatCompletion = future_result.result()
        if task_result_completion.choices[0].message.tool_calls:
            replacement_candidates.append(task_result_completion)

    if replacement_candidates:
        new_completion = summarize_multiple_completions(replacement_candidates)
        # Update chat history
        with SessionLocal() as session:
            message_replied = session.query(MessageReplied).filter(
                MessageReplied.id == chat_response.message_replied_id).first()
            if message_replied:
                message_replied.content = new_completion.choices[0].message.content.encode('utf-8')
                message_replied.tool_calls = [
                    c.dict() for c in new_completion.choices[0].message.tool_calls
                ]
            session.commit()
        chat_response.response = new_completion
        return chat_response
    else:
        return chat_response
