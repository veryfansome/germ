import asyncio
import json
import logging
from abc import ABC, abstractmethod
from openai import OpenAI
from openai.types.chat.chat_completion import ChatCompletion
from starlette.concurrency import run_in_threadpool
from typing import Optional

from api.models import ChatRequest, ChatResponse
from bot.graph.idea import idea_graph
from bot.websocket import WebSocketEventHandler, WebSocketSender
from db.models import (ChatSessionChatUserProfileLink, ChatUserProfile, SessionLocal)
from observability.annotations import measure_exec_seconds
from settings.openai_settings import (DEFAULT_CHAT_MODEL, DEFAULT_MINI_MODEL, DEFAULT_ROUTING_MODEL,
                                      ENABLED_CHAT_MODELS, ENABLED_IMAGE_MODELS, HTTPX_TIMEOUT)

logger = logging.getLogger(__name__)


class RoutableChatEventHandler(WebSocketEventHandler, ABC):

    @abstractmethod
    def get_function_name(self):
        pass

    @abstractmethod
    def get_function_settings(self):
        pass


class ChatModelEventHandler(RoutableChatEventHandler):
    def __init__(self, model: str = DEFAULT_CHAT_MODEL):
        self.function_name = f"use_{model}"
        self.function_settings = {
            "type": "function",
            "function": {
                "name": self.function_name,
                "description": " ".join((
                    f"Use {model} to generate a reply if the user asks for {model}",
                    f"or if {model} would return a better result than one generated by {DEFAULT_ROUTING_MODEL}.",
                )),
                "parameters": {},
            },
        }
        self.model = model

    def do_chat_completion(self, chat_request: ChatRequest) -> ChatCompletion:
        with OpenAI() as client:
            return client.chat.completions.create(
                messages=[message.model_dump() for message in chat_request.messages],
                model=self.model,
                n=1, temperature=chat_request.temperature,
                timeout=HTTPX_TIMEOUT)

    def get_function_name(self):
        return self.function_name

    def get_function_settings(self):
        return self.function_settings

    @measure_exec_seconds(use_logging=True, use_prometheus=True)
    async def on_receive(self,
                         chat_session_id: int, chat_request_received_id: int,
                         chat_request: ChatRequest, ws_sender: WebSocketSender):
        completion = await run_in_threadpool(self.do_chat_completion, chat_request)
        task = asyncio.create_task(ws_sender.return_chat_response(
            chat_request_received_id, ChatResponse(
                content=completion.choices[0].message.content,
                model=completion.model)))


class ImageModelEventHandler(RoutableChatEventHandler):
    def __init__(self, model: str):
        self.function_name = f"use_{model}"
        self.function_settings = {
            "type": "function",
            "function": {
                "name": self.function_name,
                "description": " ".join((
                    f"Use {model} to generate an image if {model} is likely to achieve good results.",
                )),
                "parameters": {},
            },
        }
        self.model = model

    def generate_markdown_image(self, chat_request: ChatRequest):
        image_model_inputs = generate_image_model_inputs(chat_request)
        try:
            with OpenAI() as client:
                submission = client.images.generate(
                    prompt=image_model_inputs['prompt'], model=self.model, n=1,
                    size=image_model_inputs['size'], style=image_model_inputs['style'],
                    timeout=HTTPX_TIMEOUT)
                return f"[![{image_model_inputs['prompt']}]({submission.data[0].url})]({submission.data[0].url})"
        except Exception as e:
            logger.error(e)
            return f"[![Failed to generate image](/static/oops_image.jpg)](/static/oops_image.jpg)"

    def get_function_name(self):
        return self.function_name

    def get_function_settings(self):
        return self.function_settings

    @measure_exec_seconds(use_logging=True, use_prometheus=True)
    async def on_receive(self,
                         chat_session_id: int, chat_request_received_id: int,
                         chat_request: ChatRequest, ws_sender: WebSocketSender):
        markdown_image = await run_in_threadpool(self.generate_markdown_image, chat_request)
        task = asyncio.create_task(
            ws_sender.return_chat_response(
                chat_request_received_id,
                ChatResponse(content=markdown_image, model=self.model)))


def messages_to_transcript(chat_request: ChatRequest):
    """
    GPT-4o recommended the following format because it captures directionality and provides clear boundaries to
    messages, which is helpful when trying to get embeddings that capture the context of the conversation.

    [USER] Hi! [ASSISTANT]
    [ASSISTANT] Hey! How can I help you? [USER]
    [USER] Tell me a joke [ASSISTANT]

    :param chat_request:
    :return: transcript as a string
    """
    transcript_lines = []
    for message in chat_request.messages:
        end_tag = "user" if message.role == "assistant" else "assistant"
        transcript_lines.append(f"[{message.role.upper()}] {message.content} [{end_tag.upper()}]")
    return "\n".join(transcript_lines)


@measure_exec_seconds(use_logging=True, use_prometheus=True)
def generate_image_model_inputs(chat_request: ChatRequest):
    with OpenAI() as client:
        completion = client.chat.completions.create(
            messages=([{
                "role": "system",
                "content": "The user wants an image."
            }] + [message.model_dump() for message in chat_request.messages]),

            model=DEFAULT_MINI_MODEL, n=1, temperature=chat_request.temperature,
            tool_choice={
                "type": "function",
                "function": {"name": "generate_image"}
            },
            tools=[{
                "type": "function",
                "function": {
                    "name": "generate_image",
                    "description": "Return a generated image, given a text prompt, image size, and style.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "prompt": {
                                "type": "string",
                                "description": "The text prompt for the image model."
                            },
                            "size": {
                                "type": "string",
                                "enum": ["1024x1024", "1024x1792", "1792x1024"],
                            },
                            "style": {
                                "type": "string",
                                "enum": ["natural", "vivid"],
                            },
                        },
                        "required": ["prompt", "size", "style"],
                        "additionalProperties": False,
                    },
                }
            }],
            timeout=HTTPX_TIMEOUT)
        return json.loads(completion.choices[0].message.tool_calls[0].function.arguments)


USER_FACING_CHAT_HANDLERS: dict[str, RoutableChatEventHandler] = {}
for m in ENABLED_CHAT_MODELS:
    USER_FACING_CHAT_HANDLERS[m] = ChatModelEventHandler(m)
for m in ENABLED_IMAGE_MODELS:
    USER_FACING_CHAT_HANDLERS[m] = ImageModelEventHandler(m)


class ChatRoutingEventHandler(ChatModelEventHandler):
    """
    This class is a user facing router. It returns simple completions and delegates to other handlers
    if the situation calls for other tools.
    """

    def __init__(self, model: str = DEFAULT_ROUTING_MODEL):
        super().__init__(model)
        self.model: str = model
        self.tools: dict[str, RoutableChatEventHandler] = {}
        for chat_handler in USER_FACING_CHAT_HANDLERS.values():
            self.add_tool(chat_handler.get_function_name(), chat_handler)
            logger.info(f"added {chat_handler.get_function_name()} => {chat_handler}")

    def add_tool(self, tool_name: str, chat_handler: RoutableChatEventHandler):
        self.tools[tool_name] = chat_handler

    @measure_exec_seconds(use_logging=True, use_prometheus=True)
    async def on_receive(self,
                         chat_session_id: int, chat_request_received_id: int,
                         chat_request: ChatRequest, ws_sender: WebSocketSender):
        completion = await run_in_threadpool(self.do_chat_completion, chat_request)
        if completion is None:
            task = asyncio.create_task(ws_sender.return_chat_response(
                chat_request_received_id,
                ChatResponse(content="Sorry, I'm unable to access my language model.")))
            return
        elif completion.choices[0].message.content is None:
            if completion.choices[0].message.tool_calls is not None:
                for tool_call in completion.choices[0].message.tool_calls:
                    task = asyncio.create_task(
                        self.tools[tool_call.function.name].on_receive(
                            chat_session_id, chat_request_received_id, chat_request, ws_sender
                        )
                    )
                return
            else:
                logger.error("completion content and tool_calls are both missing", completion)
                completion.choices[0].message.content = "Strange... I don't have a response"
        task = asyncio.create_task(ws_sender.return_chat_response(
            chat_request_received_id,
            ChatResponse(
                content=completion.choices[0].message.content,
                model=completion.model)))

    def do_chat_completion(self, chat_request: ChatRequest) -> Optional[ChatCompletion]:
        tools = [t.get_function_settings() for t in self.tools.values()]
        try:
            with OpenAI() as client:
                return client.chat.completions.create(
                    messages=[message.model_dump() for message in chat_request.messages],
                    model=self.model,
                    n=1, temperature=chat_request.temperature,
                    tools=tools,
                    timeout=HTTPX_TIMEOUT)
        except Exception as e:
            logger.error(e)
        return None


class UserProfileConsumer(ABC):
    @abstractmethod
    async def on_new_user_profile(self, user_profile, chat_session_id: int, chat_request_received_id: id,
                                  chat_request: ChatRequest, ws_sender: WebSocketSender):
        pass


class UserProfilingHandler(WebSocketEventHandler):
    """
    This class is an off-stage router. It analyzes conversations to construct profiles of users, which are persisted
    and linked with sessions. Additional handlers ara then activated if the situation calls for it.
    """

    tool_func_name = "store_user_profile"
    tool_properties_spec = {
        "intent": {
            "type": "string",
            "description": "A declarative sentence, clearly stating what the user wants.",
        },
        "sentiment": {
            "type": "number",
            "description": ("On a scale of 0 to 5, where 0 is very bad, 3 is neutral, and 5 is very good, "
                            "how does the user feels about the conversation?"),
        }}
    tool = {
        "type": "function",
        "function": {
            "name": tool_func_name,
            "description": "Store user profile generated from an analysis of the conversation.",
            "parameters": {
                "type": "object",
                "description": "Analysis of the user and the conversation to store.",
                "properties": tool_properties_spec,
                "required": list(tool_properties_spec.keys()),
                "additionalProperties": False,
            },
        },
    }

    def __init__(self, consumers: list[UserProfileConsumer] = None, model: str = DEFAULT_CHAT_MODEL):
        self.consumers: list[UserProfileConsumer] = consumers if consumers else []
        self.model = model

    def add_consumer(self, consumer: UserProfileConsumer):
        self.consumers.append(consumer)

    @measure_exec_seconds(use_logging=True, use_prometheus=True)
    async def on_receive(self, chat_session_id: int, chat_request_received_id: id,
                         chat_request: ChatRequest, ws_sender: WebSocketSender):
        user_profile = await run_in_threadpool(self.process_chat_request, chat_session_id, chat_request)
        await idea_graph.add_sentence(user_profile["profile"]["intent"])
        for consumer in self.consumers:
            task = asyncio.create_task(
                consumer.on_new_user_profile(
                    user_profile, chat_session_id, chat_request_received_id, chat_request, ws_sender))

    def process_chat_request(self, chat_session_id: int, chat_request: ChatRequest):
        with OpenAI() as client:
            completion = client.chat.completions.create(
                messages=[message.model_dump() for message in chat_request.messages if message.role != "system"],
                model=self.model,
                n=1, temperature=0,
                tool_choice={
                    "type": "function",
                    "function": {"name": UserProfilingHandler.tool_func_name}
                },
                tools=[UserProfilingHandler.tool],
                timeout=HTTPX_TIMEOUT)
            profile_parameters = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)
            user_profile = {
                "profile": profile_parameters,
                "system_messages": [
                    message.model_dump() for message in chat_request.messages if message.role == "system"]
            }
            with SessionLocal() as session:
                user_profile_record = ChatUserProfile(chat_user_profile=user_profile)
                session.add(user_profile_record)
                session.commit()
                user_profile_to_session_link = ChatSessionChatUserProfileLink(
                    chat_session_id=chat_session_id, chat_user_profile_id=user_profile_record.chat_user_profile_id)
                session.add(user_profile_to_session_link)
                session.commit()
            return user_profile
